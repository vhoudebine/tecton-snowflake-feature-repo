{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8baa837d-77b8-4d12-826e-d1a1ab705d5e",
   "metadata": {},
   "source": [
    "# On-Demand Feature Views (ODFVs) Tutorial\n",
    "\n",
    "Tecton has 3 basic types of Feature Views in Tecton:\n",
    "- [Batch Feature View](https://docs.tecton.ai/docs/defining-features/feature-views/batch-feature-view)\n",
    "- [Stream Feature View](https://docs.tecton.ai/docs/defining-features/feature-views/stream-feature-view)\n",
    "- [On-Demand Feature View](https://docs.tecton.ai/docs/defining-features/feature-views/on-demand-feature-view)\n",
    "\n",
    "In this tutorial we'll focus on **On-Demand Feature Views**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a5e25-40eb-449c-9432-cbf350c99f28",
   "metadata": {},
   "source": [
    "## What is an On-Demand Feature?\n",
    "\n",
    "Most of the features that you'll build in Tecton are **precomputed** -- this means that Tecton will run the data pipelines needed to compute these features before they are needed, and your ML applications will simply look up precomputed feature values from Tecton.\n",
    "\n",
    "In some scenarios, the model of precomputing features doesn't make sense, and instead you'd rather compute the value of a feature **on-demand**.  Some examples:\n",
    "* You need access to data that is only available just before you need to make a prediction\n",
    "  * (example) a user is making a transaction, and you want to compute features about the transaction\n",
    "  * (example) a user just filled out a form in your application, and you want to featurize the data they entered\n",
    "* Precomputing features is inefficient because most of the features will never be used\n",
    "  * (example) you want to calculate two users mutual friends, but precomputing mutual friends for every user is infeasible\n",
    "\n",
    "For these scenarios, Tecton has support for **On-Demand Features** -- features that are dynamically computed when requesting features for inference.  Also note that inputs for On-Demand Feature Views can be provided on the request to Tecton for feature data, as well as data retrieved from the feature store.\n",
    "\n",
    "## How do they work?\n",
    "\n",
    "### Writing On-Demand Features / Modes Available\n",
    "On-Demand Features are written in declaritive code just like all other features in Tecton.  They are written in python or pandas code depending on the code specified in the decorator.\n",
    "\n",
    "### At Inference Time\n",
    "At inference time, the transformation logic for on-demand feature are run directly on the Tecton-managed serving infrastructure. Tecton has developed an efficient method to quickly invoke python functions at serving time without inducing significant overhead. How this works:\n",
    "\n",
    "1. When you invoke [Tecton's Feature Serving API](https://docs.tecton.ai/v2/examples/fetch-real-time-features.html), you'll include any request-time data that needs to be processed in one-or-more on-demand features.\n",
    "2. While Tecton is looking up any precomputed features, Tecton will also invoke your on-demand transformation logic to compute the on-demand feature on the fly.\n",
    "3. Tecton will return a feature vector that includes both the precomputed and on-demand features that you requested from the API\n",
    "\n",
    "### At Training Time\n",
    "At training time, Tecton makes it easy to run the exact same transformation logic against your historical data.  Specifically, Tecton will turn your python transformation into a Python UDF that can efficiently run your transformation logic against large datasets.\n",
    "\n",
    "#### Speed\n",
    "Note that this is your own code and its efficiency can affect serving latency.  Also note there are two supported modes; `python` and `pandas` - the former is quickest for real-time serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941051c-a11a-4795-ad41-99fd312907a3",
   "metadata": {},
   "source": [
    "## Tutorial: Building an On-Demand Feature\n",
    "\n",
    "In this tutorial, we'll walk through a few examples of usage patterns for On-Demand Feature Views.\n",
    "We will build 2 ODFVs:\n",
    "* Credit score binning + sum of outgoing transactions based on request-time data\n",
    "* Number of days between the user's last transaction and the current transaction\n",
    "\n",
    "### 1. ODFV 1: Processing a JSON Payload on the request\n",
    "\n",
    "In this example, we will featurize some data that a client has passed to Tecton in real-time.  The client has reached out to a third party API (e.g Plaid) and received a credit score and a series of past transactions.  This data will be provided in json format. \n",
    "\n",
    "Sample data:\n",
    "<pre>\n",
    "{\n",
    "  \"TRANSACTIONS\": [\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": 141.55, \"TIMESTAMP\": \"2023-01-10 11:05:21\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": -2000.00, \"TIMESTAMP\": \"2023-01-10 13:43:09\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": 317.95, \"TIMESTAMP\": \"2023-01-10 12:27:57\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": -500.00, \"TIMESTAMP\": \"2023-01-10 19:19:32\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": 411.19, \"TIMESTAMP\": \"2023-01-10 21:51:46\"}\n",
    "  ],\n",
    "  \"CREDIT_SCORE\": 743\n",
    "}\n",
    "</pre>\n",
    "\n",
    "We will create two features from this data.\n",
    "\n",
    "1. A binary `credit_score_is_high`: 1 if the score is above 730, 0 if it is not.\n",
    "2. An aggregation `sum_of_outflows`: the sum of all the transactions below 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588ca2d-483a-48f9-b508-02ab77f1daf6",
   "metadata": {},
   "source": [
    "### Declaring Request Input and ODFV Output Schemas\n",
    "\n",
    "This feature view is going to need one input\n",
    "\n",
    "1. The json payload coming from the 3rd party API\n",
    "\n",
    "We will expect the payload data to be provided in the Tecton API call as a string, we will define a `RequestSource` object that will be used as a data source for our ODFV. The `RequestSource` specifies the expected schema of the ODFV real-time inputs. \n",
    "\n",
    "We also need to declare the schema of our output feature.  In this case, our `credit_score_is_high` is of type `Int64` while our `sum_of_outflows` feature is of type `Float64`\n",
    "\n",
    "Below, we'll use Tecton types to declare what the input request schema provides and what the output schema looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1d7d0c-397d-42f4-beb1-bb1842a4bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton import RequestSource\n",
    "from tecton.types import Float64, Int64, Field, String\n",
    "\n",
    "request_schema = [Field('payload', String)]\n",
    "transaction_request = RequestSource(schema=request_schema)\n",
    "\n",
    "output_schema = [\n",
    "  Field('credit_score_is_high', Int64),\n",
    "  Field('sum_of_outflows', Float64)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95689f-1d70-4bba-9cf8-15cf6e3b8359",
   "metadata": {},
   "source": [
    "### Defining the ODFV function\n",
    "\n",
    "\n",
    "Now we can define, validate and test our On-Demand Feature View locally in this notebook against mock inputs. For this Feature View, the mode is set to `python` which means that the input and output objects will be dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9902790e-3d13-4599-a38f-e1f29feb3ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnDemandFeatureView 'odfv_payload_features': Validating 1 dependency.\n",
      "    Transformation 'odfv_payload_features': Successfully validated.\n",
      "OnDemandFeatureView 'odfv_payload_features': Successfully validated.\n"
     ]
    }
   ],
   "source": [
    "from tecton import on_demand_feature_view\n",
    "\n",
    "@on_demand_feature_view(\n",
    "  sources=[transaction_request],\n",
    "  mode='python',\n",
    "  schema=output_schema,\n",
    ")\n",
    "def odfv_payload_features(transaction_request):\n",
    "    import json\n",
    "    import pandas\n",
    "\n",
    "    response_parsed = json.loads(transaction_request['payload'])\n",
    "\n",
    "    credit_score_is_high = 0\n",
    "    if 'CREDIT_SCORE' in response_parsed:\n",
    "        if response_parsed['CREDIT_SCORE'] > 730:\n",
    "            credit_score_is_high = 1\n",
    "  \n",
    "    sum_of_outflows = 0\n",
    "    if 'TRANSACTIONS' in response_parsed:\n",
    "        df = pandas.json_normalize(response_parsed['TRANSACTIONS'])\n",
    "        series_outflow_amounts = df[df['AMT'] < 0]['AMT']\n",
    "\n",
    "        if len(series_outflow_amounts) > 0:\n",
    "            sum_of_outflows = sum(series_outflow_amounts)\n",
    "\n",
    "    return {'credit_score_is_high': credit_score_is_high, \n",
    "    'sum_of_outflows': sum_of_outflows}\n",
    "\n",
    "odfv_payload_features.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d4035-a35a-43e2-921f-b28880197d80",
   "metadata": {},
   "source": [
    "### Testing the ODFV against mock inputs\n",
    "There are now several ways we can test this ODFV. One is by providing mock inputs and calling the `run` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d53c1a-0af6-4c37-bea7-4b1e0e7613c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "request_dict = \\\n",
    "{\n",
    "  \"TRANSACTIONS\": [\n",
    "    {\"USER_ID\": \"john\", \"AMT\": -100.00, \"TIMESTAMP\": \"2023-01-10 11:05:21\"},\n",
    "    {\"USER_ID\": \"john\", \"AMT\": -300.00, \"TIMESTAMP\": \"2023-01-10 13:43:09\"},\n",
    "    {\"USER_ID\": \"john\", \"AMT\": 23.97, \"TIMESTAMP\": \"2023-01-10 12:27:57\"}\n",
    "  ],\n",
    "  \"CREDIT_SCORE\": 691\n",
    "}\n",
    "\n",
    "request_payload = json.dumps(request_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475f1cc9-b34d-4579-8f57-2feb1611d1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'credit_score_is_high': 0, 'sum_of_outflows': -400.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odfv_payload_features.run(transaction_request={'payload': request_payload})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edcba6-3755-4159-8015-e24aa549312e",
   "metadata": {},
   "source": [
    "### 2. ODFV 2: Number of days between the user's last transaction and the current transaction\n",
    "\n",
    "On-Demand Feature Views can depend on pre-computed (Batch or Streaming) features stored in the Offline and Online store. This enables to support scenario where there's a need for combining real-time data and Batch/Streaming features. For example, comparing whether the current transaction amount is above a user's last 30 days transaction average. \n",
    "\n",
    "In our example, we will compute the number of days between a user's last transaction and the current transaction being processed. In order to compute this feature, we will need to read data in from 2 data sources:\n",
    "* The last transaction date prior to the current one can be pulled from a Batch Feature View or in some cases a Streaming feature view. In this tutorial, will create a Batch Feature View to compute this feature.\n",
    "* The current transaction date will come from real-time data, we will create a corresponding RequestSource.\n",
    "\n",
    "*❓Can we just use `current_timestamp()` for the request?*  **No**, we cannot - because this would not work when we are doing point-in-time-correct historic generation of datasets.  We want those operations to use the timestamp of the request in the past when it was made, so this is the value we must pass in.  Using something like `current_timestamp()` would break the proper math when doing historical time travel.\n",
    "\n",
    "#### 2.1) Creating the last transaction date Batch Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b04adc-e825-4142-8054-697c03b5e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 0.7.0b29\n",
      "Git Commit: 4421324c8d9880367529bc978d7fa27b044b6fa7\n",
      "Build Datetime: 2023-05-16T23:03:03\n"
     ]
    }
   ],
   "source": [
    "# Import Tecton and other libraries\n",
    "import logging\n",
    "import os\n",
    "import tecton\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "logging.getLogger('snowflake.connector').setLevel(logging.WARNING)\n",
    "logging.getLogger('snowflake.snowpark').setLevel(logging.WARNING)\n",
    "\n",
    "connection_parameters = {\n",
    "    \"user\": os.environ['SNOWFLAKE_USER'],\n",
    "    \"password\": os.environ['SNOWFLAKE_PASSWORD'],\n",
    "    \"account\": os.environ['SNOWFLAKE_ACCOUNT'],\n",
    "    \"warehouse\": \"TRIAL_WAREHOUSE\",\n",
    "    # Database and schema are required to create various temporary objects by tecton\n",
    "    \"database\": \"TECTON\",\n",
    "    \"schema\": \"PUBLIC\",\n",
    "}\n",
    "\n",
    "conn = snowflake.connector.connect(**connection_parameters)\n",
    "tecton.snowflake_context.set_connection(conn) # Tecton will use this Snowflake connection for all interactive queries\n",
    "\n",
    "\n",
    "# Quick helper function to query snowflake from a notebook\n",
    "# Make sure to replace with the appropriate connection details for your own account\n",
    "def query_snowflake(query):\n",
    "    df = conn.cursor().execute(query).fetch_pandas_all()\n",
    "    return df\n",
    "\n",
    "tecton.version.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e97bf601-90fc-48c2-b984-1b1d8fc59f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = tecton.get_workspace('prod')\n",
    "user = ws.get_entity('fraud_user')\n",
    "transactions = ws.get_data_source('transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b87c3c5-9353-470a-9cc1-41a0d043af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchFeatureView 'last_transaction': Validating 1 of 3 dependencies. (2 already validated)\n",
      "    Transformation 'last_transaction': Successfully validated.\n",
      "BatchFeatureView 'last_transaction': Successfully validated.\n"
     ]
    }
   ],
   "source": [
    "from tecton import batch_feature_view \n",
    "\n",
    "# make a BFV for transactions\n",
    "@batch_feature_view(\n",
    "  sources=[transactions],\n",
    "  entities=[user],\n",
    "  mode='snowflake_sql',\n",
    "  batch_schedule=timedelta(days=1),\n",
    "  feature_start_time=datetime(2023, 1, 1),\n",
    "  timestamp_field='TIMESTAMP',\n",
    "  ttl=timedelta(days=365)\n",
    ")\n",
    "def last_transaction(transactions_batch):\n",
    "    return f'''\n",
    "    SELECT USER_ID, \n",
    "    AMT as LAST_TRANSACTION_AMOUNT,\n",
    "    cast(TIMESTAMP as string) as LAST_TRANSACTION_TIMESTAMP,  --we need to alias timestamp and make it a string to make it a feature\n",
    "    TIMESTAMP\n",
    "    FROM {transactions_batch}\n",
    "  '''\n",
    "\n",
    "last_transaction.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df8b5b5-861d-4a42-b7ed-7abb8f2a8dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>LAST_TRANSACTION_AMOUNT</th>\n",
       "      <th>LAST_TRANSACTION_TIMESTAMP</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_26990816968</td>\n",
       "      <td>150.40</td>\n",
       "      <td>2023-01-01 00:14:43.195</td>\n",
       "      <td>2023-01-01 00:14:43.195761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_930691958107</td>\n",
       "      <td>5.38</td>\n",
       "      <td>2023-01-01 00:14:47.045</td>\n",
       "      <td>2023-01-01 00:14:47.045653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_687958452057</td>\n",
       "      <td>70.87</td>\n",
       "      <td>2023-01-01 00:14:50.914</td>\n",
       "      <td>2023-01-01 00:14:50.914808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_724235628997</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2023-01-01 00:14:54.851</td>\n",
       "      <td>2023-01-01 00:14:54.851489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_502567604689</td>\n",
       "      <td>74.51</td>\n",
       "      <td>2023-01-01 00:14:56.807</td>\n",
       "      <td>2023-01-01 00:14:56.807652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             USER_ID  LAST_TRANSACTION_AMOUNT LAST_TRANSACTION_TIMESTAMP  \\\n",
       "0   user_26990816968                   150.40    2023-01-01 00:14:43.195   \n",
       "1  user_930691958107                     5.38    2023-01-01 00:14:47.045   \n",
       "2  user_687958452057                    70.87    2023-01-01 00:14:50.914   \n",
       "3  user_724235628997                     2.44    2023-01-01 00:14:54.851   \n",
       "4  user_502567604689                    74.51    2023-01-01 00:14:56.807   \n",
       "\n",
       "                   TIMESTAMP  \n",
       "0 2023-01-01 00:14:43.195761  \n",
       "1 2023-01-01 00:14:47.045653  \n",
       "2 2023-01-01 00:14:50.914808  \n",
       "3 2023-01-01 00:14:54.851489  \n",
       "4 2023-01-01 00:14:56.807652  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_transaction.get_historical_features(\n",
    "    start_time=datetime(2023, 1, 1), \n",
    "    end_time=datetime(2023, 6, 1)).to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb965b82-b3cf-4054-9c53-4cd681ca7502",
   "metadata": {},
   "source": [
    "#### 2.2) Creating the On-Demand Feature View\n",
    "\n",
    "💡 Notice how the last_transaction BFV we defined earlier is now used as a source to the ODFV. Tecton will automatically look-up the right feature value based on the entity key provided in the request to Tecton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c539d0f-36a7-4231-97c2-d43dce0aa0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnDemandFeatureView 'odfv_days_since_last_txn': Validating 1 of 2 dependencies. (1 already validated)\n",
      "    Transformation 'odfv_days_since_last_txn': Successfully validated.\n",
      "OnDemandFeatureView 'odfv_days_since_last_txn': Successfully validated.\n"
     ]
    }
   ],
   "source": [
    "request_schema = [Field('REQUEST_TIMESTAMP', String)]\n",
    "request = RequestSource(schema=request_schema)\n",
    "output_schema = [Field('DAYS_SINCE_LAST_TRANSACTION', Int64)]\n",
    "\n",
    "@on_demand_feature_view(\n",
    "    sources=[request, last_transaction],\n",
    "    mode='python',\n",
    "    schema=output_schema\n",
    ")\n",
    "def odfv_days_since_last_txn(request, last_transaction):\n",
    "    from datetime import datetime, date\n",
    "  \n",
    "  # if we have a value from the feature store, convert the retrieved value and request date strings to dates and return the number of days between them\n",
    "    if last_transaction['LAST_TRANSACTION_TIMESTAMP']:\n",
    "        request_datetime = datetime.strptime(request['REQUEST_TIMESTAMP'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        transaction_datetime = datetime.strptime(last_transaction['LAST_TRANSACTION_TIMESTAMP'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        td = request_datetime - transaction_datetime\n",
    "        return {'DAYS_SINCE_LAST_TRANSACTION': td.days}\n",
    "  \n",
    "  # else return -1 indicating we haven't had a prior transaction\n",
    "    else:\n",
    "        return {'DAYS_SINCE_LAST_TRANSACTION': -1}\n",
    "    \n",
    "odfv_days_since_last_txn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8ad37-0730-4ebb-8fd3-dd388a9f83e0",
   "metadata": {},
   "source": [
    "✅ We can test this On-demand Feature View with mock inputs, refer to our documentation for more details on [interactive testing of ODFVs with dependencies](https://docs.tecton.ai/docs/testing-features/interactive-testing/testing-on-demand-features#on-demand-feature-views-with-feature-view-dependencies) \n",
    "\n",
    "When testing this ODFV, we have to provide mock inputs for all the ODFV inputs, including the `last_transaction` BFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf27e6af-ccb2-4481-ab8c-03f5fe6bec01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DAYS_SINCE_LAST_TRANSACTION': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odfv_days_since_last_txn.run(\n",
    "    request={'REQUEST_TIMESTAMP': '2023-05-28 00:00:00.000'}, \n",
    "    last_transaction={'LAST_TRANSACTION_TIMESTAMP': '2023-05-17 00:00:00.000'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b9092-1662-48db-bf04-6462b1087663",
   "metadata": {},
   "source": [
    "We can also test the ODFV against a spine of training events using `get_historical_features()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732e2c5-28f6-440f-b3e6-5c79f94f1fd1",
   "metadata": {},
   "source": [
    "### 3) Publishing the ODFVs to Tecton \n",
    "✅  To add these features to Tecton, simply add it to a new file in your Tecton Feature Repository and run `tecton plan` and `tecton apply`.\n",
    "We can now retrieve these feature views in our notebook and generate training data from a spine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3d82372-4e27-44e9-8192-ea17237eda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_query = '''\n",
    "SELECT \n",
    "    MERCHANT,\n",
    "    USER_ID,\n",
    "    CATEGORY,\n",
    "    TIMESTAMP,\n",
    "    cast(TIMESTAMP as string) as REQUEST_TIMESTAMP,\n",
    "    IS_FRAUD\n",
    "FROM \n",
    "    TECTON_DEMO_DATA.FRAUD_DEMO.TRANSACTIONS \n",
    "ORDER BY TIMESTAMP DESC\n",
    "LIMIT 100\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d035b13c-7fbd-4db9-bd28-43d148a9c9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>MERCHANT</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>REQUEST_TIMESTAMP</th>\n",
       "      <th>IS_FRAUD</th>\n",
       "      <th>DAYS_SINCE_LAST_TRANSACTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_26990816968</td>\n",
       "      <td>2023-07-19 01:01:04.898662</td>\n",
       "      <td>fraud_Kutch LLC</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>2023-07-19 01:01:04.898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_650387977076</td>\n",
       "      <td>2023-07-19 01:00:57.611783</td>\n",
       "      <td>fraud_Lemke-Gutmann</td>\n",
       "      <td>shopping_net</td>\n",
       "      <td>2023-07-19 01:00:57.611</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_205125746682</td>\n",
       "      <td>2023-07-19 01:00:55.625969</td>\n",
       "      <td>fraud_Kub-Heaney</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>2023-07-19 01:00:55.625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_499975010057</td>\n",
       "      <td>2023-07-19 01:00:53.483589</td>\n",
       "      <td>fraud_Fadel-Hilpert</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>2023-07-19 01:00:53.483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_268514844966</td>\n",
       "      <td>2023-07-19 01:00:51.763253</td>\n",
       "      <td>fraud_Reichel Inc</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>2023-07-19 01:00:51.763</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user_461615966685</td>\n",
       "      <td>2023-07-19 01:00:49.868651</td>\n",
       "      <td>fraud_Prosacco, Kreiger and Kovacek</td>\n",
       "      <td>home</td>\n",
       "      <td>2023-07-19 01:00:49.868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_699668125818</td>\n",
       "      <td>2023-07-19 01:00:47.462553</td>\n",
       "      <td>fraud_Heller-Langosh</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>2023-07-19 01:00:47.462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user_222506789984</td>\n",
       "      <td>2023-07-19 01:00:45.380058</td>\n",
       "      <td>fraud_Hilpert-Conroy</td>\n",
       "      <td>kids_pets</td>\n",
       "      <td>2023-07-19 01:00:45.380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user_402539845901</td>\n",
       "      <td>2023-07-19 01:00:41.320065</td>\n",
       "      <td>fraud_Brown, Homenick and Lesch</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>2023-07-19 01:00:41.320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>user_459842889956</td>\n",
       "      <td>2023-07-19 01:00:39.376894</td>\n",
       "      <td>fraud_Osinski, Ledner and Leuschke</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>2023-07-19 01:00:39.376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             USER_ID                  TIMESTAMP  \\\n",
       "0   user_26990816968 2023-07-19 01:01:04.898662   \n",
       "1  user_650387977076 2023-07-19 01:00:57.611783   \n",
       "2  user_205125746682 2023-07-19 01:00:55.625969   \n",
       "3  user_499975010057 2023-07-19 01:00:53.483589   \n",
       "4  user_268514844966 2023-07-19 01:00:51.763253   \n",
       "5  user_461615966685 2023-07-19 01:00:49.868651   \n",
       "6  user_699668125818 2023-07-19 01:00:47.462553   \n",
       "7  user_222506789984 2023-07-19 01:00:45.380058   \n",
       "8  user_402539845901 2023-07-19 01:00:41.320065   \n",
       "9  user_459842889956 2023-07-19 01:00:39.376894   \n",
       "\n",
       "                              MERCHANT        CATEGORY  \\\n",
       "0                      fraud_Kutch LLC   gas_transport   \n",
       "1                  fraud_Lemke-Gutmann    shopping_net   \n",
       "2                     fraud_Kub-Heaney  health_fitness   \n",
       "3                  fraud_Fadel-Hilpert  health_fitness   \n",
       "4                    fraud_Reichel Inc   entertainment   \n",
       "5  fraud_Prosacco, Kreiger and Kovacek            home   \n",
       "6                 fraud_Heller-Langosh   gas_transport   \n",
       "7                 fraud_Hilpert-Conroy       kids_pets   \n",
       "8      fraud_Brown, Homenick and Lesch  health_fitness   \n",
       "9   fraud_Osinski, Ledner and Leuschke     grocery_pos   \n",
       "\n",
       "         REQUEST_TIMESTAMP  IS_FRAUD  DAYS_SINCE_LAST_TRANSACTION  \n",
       "0  2023-07-19 01:01:04.898         0                            1  \n",
       "1  2023-07-19 01:00:57.611         0                            0  \n",
       "2  2023-07-19 01:00:55.625         0                            0  \n",
       "3  2023-07-19 01:00:53.483         0                            0  \n",
       "4  2023-07-19 01:00:51.763         0                            0  \n",
       "5  2023-07-19 01:00:49.868         0                            0  \n",
       "6  2023-07-19 01:00:47.462         0                            0  \n",
       "7  2023-07-19 01:00:45.380         0                            0  \n",
       "8  2023-07-19 01:00:41.320         0                            0  \n",
       "9  2023-07-19 01:00:39.376         0                            0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fv = tecton.get_workspace('YOUR_WORKSPACE').get_feature_view('odfv_days_since_last_txn')\n",
    "fv.get_historical_features(transactions_query).to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17327962-90ac-4b1f-8c06-e662b83f3399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
