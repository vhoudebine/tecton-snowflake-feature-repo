{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8baa837d-77b8-4d12-826e-d1a1ab705d5e",
   "metadata": {},
   "source": [
    "# On-Demand Feature Views (ODFVs) Tutorial\n",
    "\n",
    "Tecton has 3 basic types of Feature Views in Tecton:\n",
    "- [Batch Feature View](https://docs.tecton.ai/docs/defining-features/feature-views/batch-feature-view)\n",
    "- [Stream Feature View](https://docs.tecton.ai/docs/defining-features/feature-views/stream-feature-view)\n",
    "- [On-Demand Feature View](https://docs.tecton.ai/docs/defining-features/feature-views/on-demand-feature-view)\n",
    "\n",
    "In this tutorial we'll focus on **On-Demand Feature Views**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654531eb-e236-4b83-a38f-11e834147645",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: snowflake-snowpark-python[pandas] in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python[pandas]) (67.7.2)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python[pandas]) (0.40.0)\n",
      "Requirement already satisfied: cloudpickle<=2.0.0,>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python[pandas]) (2.0.0)\n",
      "Requirement already satisfied: snowflake-connector-python<4.0.0,>=3.0.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python[pandas]) (3.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python[pandas]) (5.4.1)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (1.15.1)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (38.0.4)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (22.1.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (3.18.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2.8.0)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2023.3)\n",
      "Requirement already satisfied: requests<3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2.29.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2023.5.7)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (4.5.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (3.12.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<3.9.0,>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (3.5.0)\n",
      "Requirement already satisfied: tomlkit in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (0.11.8)\n",
      "Requirement already satisfied: pandas<2.1.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (1.5.3)\n",
      "Collecting pyarrow<10.1.0,>=10.0.1 (from snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas])\n",
      "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2.21)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<2.1.0,>=1.0.0->snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<2.1.0,>=1.0.0->snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (1.22.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<2.1.0,>=1.0.0->snowflake-connector-python<4.0.0,>=3.0.4->snowflake-snowpark-python[pandas]) (1.16.0)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 8.0.0\n",
      "    Uninstalling pyarrow-8.0.0:\n",
      "      Successfully uninstalled pyarrow-8.0.0\n",
      "Successfully installed pyarrow-10.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-snowpark-python[pandas]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a5e25-40eb-449c-9432-cbf350c99f28",
   "metadata": {},
   "source": [
    "## What is an On-Demand Feature?\n",
    "\n",
    "Most of the features that you'll build in Tecton are **precomputed** -- this means that Tecton will run the data pipelines needed to compute these features before they are needed, and your ML applications will simply look up precomputed feature values from Tecton.\n",
    "\n",
    "In some scenarios, the model of precomputing features doesn't make sense, and instead you'd rather compute the value of a feature **on-demand**.  Some examples:\n",
    "* You need access to data that is only available just before you need to make a prediction\n",
    "  * (example) a user is making a transaction, and you want to compute features about the transaction\n",
    "  * (example) a user just filled out a form in your application, and you want to featurize the data they entered\n",
    "* Precomputing features is inefficient because most of the features will never be used\n",
    "  * (example) you want to calculate two users mutual friends, but precomputing mutual friends for every user is infeasible\n",
    "\n",
    "For these scenarios, Tecton has support for **On-Demand Features** -- features that are dynamically computed when requesting features for inference.  Also note that inputs for On-Demand Feature Views can be provided on the request to Tecton for feature data, as well as data retrieved from the feature store.\n",
    "\n",
    "## How do they work?\n",
    "\n",
    "### Writing On-Demand Features / Modes Available\n",
    "On-Demand Features are written in declaritive code just like all other features in Tecton.  They are written in python or pandas code depending on the code specified in the decorator.\n",
    "\n",
    "### At Inference Time\n",
    "At inference time, the transformation logic for on-demand feature are run directly on the Tecton-managed serving infrastructure. Tecton has developed an efficient method to quickly invoke python functions at serving time without inducing significant overhead. How this works:\n",
    "\n",
    "1. When you invoke [Tecton's Feature Serving API](https://docs.tecton.ai/v2/examples/fetch-real-time-features.html), you'll include any request-time data that needs to be processed in one-or-more on-demand features.\n",
    "2. While Tecton is looking up any precomputed features, Tecton will also invoke your on-demand transformation logic to compute the on-demand feature on the fly.\n",
    "3. Tecton will return a feature vector that includes both the precomputed and on-demand features that you requested from the API\n",
    "\n",
    "### At Training Time\n",
    "At training time, Tecton makes it easy to run the exact same transformation logic against your historical data.  Specifically, Tecton will turn your python transformation into a Python UDF that can efficiently run your transformation logic against large datasets.\n",
    "\n",
    "#### Speed\n",
    "Note that this is your own code and its efficiency can affect serving latency.  Also note there are two supported modes; `python` and `pandas` - the former is quickest for real-time serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941051c-a11a-4795-ad41-99fd312907a3",
   "metadata": {},
   "source": [
    "## Tutorial: Building an On-Demand Feature\n",
    "\n",
    "In this tutorial, we'll walk through a few examples of usage patterns for On-Demand Feature Views.\n",
    "We will build 2 ODFVs:\n",
    "* Credit score binning + sum of outgoing transactions based on request-time data\n",
    "* Number of days between the user's last transaction and the current transaction\n",
    "\n",
    "### 1. ODFV 1: Processing a JSON Payload on the request\n",
    "\n",
    "In this example, we will featurize some data that a client has passed to Tecton in real-time.  The client has reached out to a third party API (e.g Plaid) and received a credit score and a series of past transactions.  This data will be provided in json format. \n",
    "\n",
    "Sample data:\n",
    "<pre>\n",
    "{\n",
    "  \"TRANSACTIONS\": [\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": 141.55, \"TIMESTAMP\": \"2023-01-10 11:05:21\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": -2000.00, \"TIMESTAMP\": \"2023-01-10 13:43:09\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": 317.95, \"TIMESTAMP\": \"2023-01-10 12:27:57\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": -500.00, \"TIMESTAMP\": \"2023-01-10 19:19:32\"},\n",
    "    {\"USER_ID\": \"miket\", \"AMT\": 411.19, \"TIMESTAMP\": \"2023-01-10 21:51:46\"}\n",
    "  ],\n",
    "  \"CREDIT_SCORE\": 743\n",
    "}\n",
    "</pre>\n",
    "\n",
    "We will create two features from this data.\n",
    "\n",
    "1. A binary `credit_score_is_high`: 1 if the score is above 730, 0 if it is not.\n",
    "2. An aggregation `sum_of_outflows`: the sum of all the transactions below 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588ca2d-483a-48f9-b508-02ab77f1daf6",
   "metadata": {},
   "source": [
    "### Declaring Request Input and ODFV Output Schemas\n",
    "\n",
    "This feature view is going to need one input\n",
    "\n",
    "1. The json payload coming from the 3rd party API\n",
    "\n",
    "We will expect the payload data to be provided in the Tecton API call as a string, we will define a `RequestSource` object that will be used as a data source for our ODFV. The `RequestSource` specifies the expected schema of the ODFV real-time inputs. \n",
    "\n",
    "We also need to declare the schema of our output feature.  In this case, our `credit_score_is_high` is of type `Int64` while our `sum_of_outflows` feature is of type `Float64`\n",
    "\n",
    "Below, we'll use Tecton types to declare what the input request schema provides and what the output schema looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1d7d0c-397d-42f4-beb1-bb1842a4bd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tecton import RequestSource\n",
    "from tecton.types import Float64, Int64, Field, String\n",
    "\n",
    "request_schema = [Field('payload', String)]\n",
    "transaction_request = RequestSource(schema=request_schema)\n",
    "\n",
    "output_schema = [\n",
    "  Field('credit_score_is_high', Int64),\n",
    "  Field('sum_of_outflows', Float64)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95689f-1d70-4bba-9cf8-15cf6e3b8359",
   "metadata": {},
   "source": [
    "### Defining the ODFV function\n",
    "\n",
    "\n",
    "Now we can define, validate and test our On-Demand Feature View locally in this notebook against mock inputs. For this Feature View, the mode is set to `python` which means that the input and output objects will be dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9902790e-3d13-4599-a38f-e1f29feb3ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnDemandFeatureView 'odfv_payload_features': Validating 1 dependency.\n",
      "    Transformation 'odfv_payload_features': Successfully validated.\n",
      "OnDemandFeatureView 'odfv_payload_features': Successfully validated.\n"
     ]
    }
   ],
   "source": [
    "from tecton import on_demand_feature_view\n",
    "\n",
    "@on_demand_feature_view(\n",
    "  sources=[transaction_request],\n",
    "  mode='python',\n",
    "  schema=output_schema,\n",
    ")\n",
    "def odfv_payload_features(transaction_request):\n",
    "    import json\n",
    "    import pandas\n",
    "\n",
    "    response_parsed = json.loads(transaction_request['payload'])\n",
    "\n",
    "    credit_score_is_high = 0\n",
    "    if 'CREDIT_SCORE' in response_parsed:\n",
    "        if response_parsed['CREDIT_SCORE'] > 730:\n",
    "            credit_score_is_high = 1\n",
    "  \n",
    "    sum_of_outflows = 0\n",
    "    if 'TRANSACTIONS' in response_parsed:\n",
    "        df = pandas.json_normalize(response_parsed['TRANSACTIONS'])\n",
    "        series_outflow_amounts = df[df['AMT'] < 0]['AMT']\n",
    "\n",
    "        if len(series_outflow_amounts) > 0:\n",
    "            sum_of_outflows = sum(series_outflow_amounts)\n",
    "\n",
    "    return {'credit_score_is_high': credit_score_is_high, \n",
    "    'sum_of_outflows': sum_of_outflows}\n",
    "\n",
    "odfv_payload_features.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d4035-a35a-43e2-921f-b28880197d80",
   "metadata": {},
   "source": [
    "### Testing the ODFV against mock inputs\n",
    "There are now several ways we can test this ODFV. One is by providing mock inputs and calling the `run` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d53c1a-0af6-4c37-bea7-4b1e0e7613c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "request_dict = \\\n",
    "{\n",
    "  \"TRANSACTIONS\": [\n",
    "    {\"USER_ID\": \"john\", \"AMT\": -100.00, \"TIMESTAMP\": \"2023-01-10 11:05:21\"},\n",
    "    {\"USER_ID\": \"john\", \"AMT\": -300.00, \"TIMESTAMP\": \"2023-01-10 13:43:09\"},\n",
    "    {\"USER_ID\": \"john\", \"AMT\": 23.97, \"TIMESTAMP\": \"2023-01-10 12:27:57\"}\n",
    "  ],\n",
    "  \"CREDIT_SCORE\": 691\n",
    "}\n",
    "\n",
    "request_payload = json.dumps(request_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475f1cc9-b34d-4579-8f57-2feb1611d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'credit_score_is_high': 0, 'sum_of_outflows': -400.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odfv_payload_features.run(transaction_request={'payload': request_payload})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edcba6-3755-4159-8015-e24aa549312e",
   "metadata": {},
   "source": [
    "### 2. ODFV 2: Number of days between the user's last transaction and the current transaction\n",
    "\n",
    "On-Demand Feature Views can depend on pre-computed (Batch or Streaming) features stored in the Offline and Online store. This enables to support scenario where there's a need for combining real-time data and Batch/Streaming features. For example, comparing whether the current transaction amount is above a user's last 30 days transaction average. \n",
    "\n",
    "In our example, we will compute the number of days between a user's last transaction and the current transaction being processed. In order to compute this feature, we will need to read data in from 2 data sources:\n",
    "* The last transaction date prior to the current one can be pulled from a Batch Feature View or in some cases a Streaming feature view. In this tutorial, will create a Batch Feature View to compute this feature.\n",
    "* The current transaction date will come from real-time data, we will create a corresponding RequestSource.\n",
    "\n",
    "*❓Can we just use `current_timestamp()` for the request?*  **No**, we cannot - because this would not work when we are doing point-in-time-correct historic generation of datasets.  We want those operations to use the timestamp of the request in the past when it was made, so this is the value we must pass in.  Using something like `current_timestamp()` would break the proper math when doing historical time travel.\n",
    "\n",
    "#### 2.1) Creating the last transaction date Batch Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b04adc-e825-4142-8054-697c03b5e33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SNOWFLAKE_USER=DEMO_USER\n",
      "env: SNOWFLAKE_PASSWORD=tecton123!\n",
      "env: SNOWFLAKE_ACCOUNT=tectonpartner-tecton_demo_usaa\n",
      "Version: 0.7.2\n",
      "Git Commit: b1f0847f4a680bb6307d53150af3589d3fad57e0\n",
      "Build Datetime: 2023-07-28T21:12:30\n"
     ]
    }
   ],
   "source": [
    "#Details were sent in an email\n",
    "%env SNOWFLAKE_USER=DEMO_USER\n",
    "%env SNOWFLAKE_PASSWORD=tecton123!\n",
    "%env SNOWFLAKE_ACCOUNT=tectonpartner-tecton_demo_usaa\n",
    "\n",
    "# Import Tecton and other libraries\n",
    "import logging\n",
    "import os\n",
    "import tecton\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "logging.getLogger('snowflake.connector').setLevel(logging.WARNING)\n",
    "logging.getLogger('snowflake.snowpark').setLevel(logging.WARNING)\n",
    "\n",
    "connection_parameters = {\n",
    "    \"user\": os.environ['SNOWFLAKE_USER'],\n",
    "    \"password\": os.environ['SNOWFLAKE_PASSWORD'],\n",
    "    \"account\": os.environ['SNOWFLAKE_ACCOUNT'],\n",
    "    \"warehouse\": \"TRIAL_WAREHOUSE\",\n",
    "    # Database and schema are required to create various temporary objects by tecton\n",
    "    \"database\": \"USAA_DEMO\",\n",
    "    \"schema\": \"PUBLIC\",\n",
    "}\n",
    "\n",
    "conn = snowflake.connector.connect(**connection_parameters)\n",
    "tecton.snowflake_context.set_connection(conn) # Tecton will use this Snowflake connection for all interactive queries\n",
    "\n",
    "\n",
    "# Quick helper function to query snowflake from a notebook\n",
    "# Make sure to replace with the appropriate connection details for your own account\n",
    "def query_snowflake(query):\n",
    "    df = conn.cursor().execute(query).fetch_pandas_all()\n",
    "    return df\n",
    "\n",
    "tecton.version.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97bf601-90fc-48c2-b984-1b1d8fc59f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws = tecton.get_workspace('prod')\n",
    "user = ws.get_entity('fraud_user')\n",
    "transactions = ws.get_data_source('transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b87c3c5-9353-470a-9cc1-41a0d043af6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchFeatureView 'last_transaction': Validating 1 of 3 dependencies. (2 already validated)\n",
      "    Transformation 'last_transaction': Successfully validated.\n",
      "BatchFeatureView 'last_transaction': Successfully validated.\n"
     ]
    }
   ],
   "source": [
    "from tecton import batch_feature_view \n",
    "\n",
    "# make a BFV for transactions\n",
    "@batch_feature_view(\n",
    "  sources=[transactions],\n",
    "  entities=[user],\n",
    "  mode='snowflake_sql',\n",
    "  batch_schedule=timedelta(days=1),\n",
    "  feature_start_time=datetime(2023, 1, 1),\n",
    "  timestamp_field='TIMESTAMP',\n",
    "  ttl=timedelta(days=365)\n",
    ")\n",
    "def last_transaction(transactions_batch):\n",
    "    return f'''\n",
    "    SELECT USER_ID, \n",
    "    AMT as LAST_TRANSACTION_AMOUNT,\n",
    "    cast(TIMESTAMP as string) as LAST_TRANSACTION_TIMESTAMP,  --we need to alias timestamp and make it a string to make it a feature\n",
    "    TIMESTAMP\n",
    "    FROM {transactions_batch}\n",
    "  '''\n",
    "\n",
    "last_transaction.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df8b5b5-861d-4a42-b7ed-7abb8f2a8dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>LAST_TRANSACTION_AMOUNT</th>\n",
       "      <th>LAST_TRANSACTION_TIMESTAMP</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_402539845901</td>\n",
       "      <td>9.71</td>\n",
       "      <td>2023-01-01 02:35:31.588</td>\n",
       "      <td>2023-01-01 02:35:31.588860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_222506789984</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2023-01-01 02:35:34.274</td>\n",
       "      <td>2023-01-01 02:35:34.274670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_502567604689</td>\n",
       "      <td>53.97</td>\n",
       "      <td>2023-01-01 02:35:36.896</td>\n",
       "      <td>2023-01-01 02:35:36.896390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_538895124917</td>\n",
       "      <td>516.54</td>\n",
       "      <td>2023-01-01 02:35:38.574</td>\n",
       "      <td>2023-01-01 02:35:38.574367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_268514844966</td>\n",
       "      <td>227.80</td>\n",
       "      <td>2023-01-01 02:35:41.087</td>\n",
       "      <td>2023-01-01 02:35:41.087538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             USER_ID  LAST_TRANSACTION_AMOUNT LAST_TRANSACTION_TIMESTAMP  \\\n",
       "0  user_402539845901                     9.71    2023-01-01 02:35:31.588   \n",
       "1  user_222506789984                     5.17    2023-01-01 02:35:34.274   \n",
       "2  user_502567604689                    53.97    2023-01-01 02:35:36.896   \n",
       "3  user_538895124917                   516.54    2023-01-01 02:35:38.574   \n",
       "4  user_268514844966                   227.80    2023-01-01 02:35:41.087   \n",
       "\n",
       "                   TIMESTAMP  \n",
       "0 2023-01-01 02:35:31.588860  \n",
       "1 2023-01-01 02:35:34.274670  \n",
       "2 2023-01-01 02:35:36.896390  \n",
       "3 2023-01-01 02:35:38.574367  \n",
       "4 2023-01-01 02:35:41.087538  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_transaction.get_historical_features(\n",
    "    start_time=datetime(2023, 1, 1), \n",
    "    end_time=datetime(2023, 6, 1)).to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb965b82-b3cf-4054-9c53-4cd681ca7502",
   "metadata": {},
   "source": [
    "#### 2.2) Creating the On-Demand Feature View\n",
    "\n",
    "💡 Notice how the last_transaction BFV we defined earlier is now used as a source to the ODFV. Tecton will automatically look-up the right feature value based on the entity key provided in the request to Tecton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c539d0f-36a7-4231-97c2-d43dce0aa0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnDemandFeatureView 'odfv_days_since_last_txn': Validating 1 of 2 dependencies. (1 already validated)\n",
      "    Transformation 'odfv_days_since_last_txn': Successfully validated.\n",
      "OnDemandFeatureView 'odfv_days_since_last_txn': Successfully validated.\n"
     ]
    }
   ],
   "source": [
    "request_schema = [Field('REQUEST_TIMESTAMP', String)]\n",
    "request = RequestSource(schema=request_schema)\n",
    "output_schema = [Field('DAYS_SINCE_LAST_TRANSACTION', Int64)]\n",
    "\n",
    "@on_demand_feature_view(\n",
    "    sources=[request, last_transaction],\n",
    "    mode='python',\n",
    "    schema=output_schema\n",
    ")\n",
    "def odfv_days_since_last_txn(request, last_transaction):\n",
    "    from datetime import datetime, date\n",
    "  \n",
    "  # if we have a value from the feature store, convert the retrieved value and request date strings to dates and return the number of days between them\n",
    "    if last_transaction['LAST_TRANSACTION_TIMESTAMP']:\n",
    "        request_datetime = datetime.strptime(request['REQUEST_TIMESTAMP'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        transaction_datetime = datetime.strptime(last_transaction['LAST_TRANSACTION_TIMESTAMP'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        td = request_datetime - transaction_datetime\n",
    "        return {'DAYS_SINCE_LAST_TRANSACTION': td.days}\n",
    "  \n",
    "  # else return -1 indicating we haven't had a prior transaction\n",
    "    else:\n",
    "        return {'DAYS_SINCE_LAST_TRANSACTION': -1}\n",
    "    \n",
    "odfv_days_since_last_txn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8ad37-0730-4ebb-8fd3-dd388a9f83e0",
   "metadata": {},
   "source": [
    "✅ We can test this On-demand Feature View with mock inputs, refer to our documentation for more details on [interactive testing of ODFVs with dependencies](https://docs.tecton.ai/docs/testing-features/interactive-testing/testing-on-demand-features#on-demand-feature-views-with-feature-view-dependencies) \n",
    "\n",
    "When testing this ODFV, we have to provide mock inputs for all the ODFV inputs, including the `last_transaction` BFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf27e6af-ccb2-4481-ab8c-03f5fe6bec01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DAYS_SINCE_LAST_TRANSACTION': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odfv_days_since_last_txn.run(\n",
    "    request={'REQUEST_TIMESTAMP': '2023-05-28 00:00:00.000'}, \n",
    "    last_transaction={'LAST_TRANSACTION_TIMESTAMP': '2023-05-17 00:00:00.000'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b9092-1662-48db-bf04-6462b1087663",
   "metadata": {},
   "source": [
    "We can also test the ODFV against a spine of training events using `get_historical_features()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732e2c5-28f6-440f-b3e6-5c79f94f1fd1",
   "metadata": {},
   "source": [
    "### 3) Publishing the ODFVs to Tecton \n",
    "✅  To add these features to Tecton, simply add it to a new file in your Tecton Feature Repository and run `tecton plan` and `tecton apply`.\n",
    "We can now retrieve these feature views in our notebook and generate training data from a spine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d82372-4e27-44e9-8192-ea17237eda07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MERCHANT</th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>AMT</th>\n",
       "      <th>REQUEST_TIMESTAMP</th>\n",
       "      <th>IS_FRAUD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fraud_Haley, Jewess and Bechtelar</td>\n",
       "      <td>user_459842889956</td>\n",
       "      <td>shopping_pos</td>\n",
       "      <td>2023-08-03 20:55:36.156244</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2023-08-03 20:55:36.156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fraud_Hamill-D'Amore</td>\n",
       "      <td>user_650387977076</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>2023-08-03 20:55:33.612956</td>\n",
       "      <td>53.08</td>\n",
       "      <td>2023-08-03 20:55:33.612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fraud_Durgan-Auer</td>\n",
       "      <td>user_394495759023</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>2023-08-03 20:55:30.557317</td>\n",
       "      <td>5.57</td>\n",
       "      <td>2023-08-03 20:55:30.557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fraud_McCullough, Hudson and Schuster</td>\n",
       "      <td>user_461615966685</td>\n",
       "      <td>food_dining</td>\n",
       "      <td>2023-08-03 20:55:27.434522</td>\n",
       "      <td>35.37</td>\n",
       "      <td>2023-08-03 20:55:27.434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fraud_Bernhard Inc</td>\n",
       "      <td>user_884240387242</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>2023-08-03 20:55:25.250398</td>\n",
       "      <td>78.23</td>\n",
       "      <td>2023-08-03 20:55:25.250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                MERCHANT            USER_ID        CATEGORY  \\\n",
       "0      fraud_Haley, Jewess and Bechtelar  user_459842889956    shopping_pos   \n",
       "1                   fraud_Hamill-D'Amore  user_650387977076  health_fitness   \n",
       "2                      fraud_Durgan-Auer  user_394495759023        misc_net   \n",
       "3  fraud_McCullough, Hudson and Schuster  user_461615966685     food_dining   \n",
       "4                     fraud_Bernhard Inc  user_884240387242   gas_transport   \n",
       "\n",
       "                   TIMESTAMP    AMT        REQUEST_TIMESTAMP IS_FRAUD  \n",
       "0 2023-08-03 20:55:36.156244   1.92  2023-08-03 20:55:36.156        0  \n",
       "1 2023-08-03 20:55:33.612956  53.08  2023-08-03 20:55:33.612        0  \n",
       "2 2023-08-03 20:55:30.557317   5.57  2023-08-03 20:55:30.557        0  \n",
       "3 2023-08-03 20:55:27.434522  35.37  2023-08-03 20:55:27.434        0  \n",
       "4 2023-08-03 20:55:25.250398  78.23  2023-08-03 20:55:25.250        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_query = '''\n",
    "SELECT \n",
    "    MERCHANT,\n",
    "    USER_ID,\n",
    "    CATEGORY,\n",
    "    TIMESTAMP,\n",
    "    AMT,\n",
    "    cast(TIMESTAMP as string) as REQUEST_TIMESTAMP,\n",
    "    IS_FRAUD\n",
    "FROM \n",
    "    TECTON_DEMO_DATA.FRAUD_DEMO.TRANSACTIONS \n",
    "ORDER BY TIMESTAMP DESC\n",
    "LIMIT 10\n",
    "'''\n",
    "transactions = query_snowflake(transactions_query)\n",
    "transactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d035b13c-7fbd-4db9-bd28-43d148a9c9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MERCHANT</th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>AMT</th>\n",
       "      <th>REQUEST_TIMESTAMP</th>\n",
       "      <th>IS_FRAUD</th>\n",
       "      <th>TRANSACTION_AMOUNT_IS_HIGH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fraud_Haley, Jewess and Bechtelar</td>\n",
       "      <td>user_459842889956</td>\n",
       "      <td>shopping_pos</td>\n",
       "      <td>2023-08-03 20:55:36.156244</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2023-08-03 20:55:36.156</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fraud_Hamill-D'Amore</td>\n",
       "      <td>user_650387977076</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>2023-08-03 20:55:33.612956</td>\n",
       "      <td>53.08</td>\n",
       "      <td>2023-08-03 20:55:33.612</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fraud_Durgan-Auer</td>\n",
       "      <td>user_394495759023</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>2023-08-03 20:55:30.557317</td>\n",
       "      <td>5.57</td>\n",
       "      <td>2023-08-03 20:55:30.557</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fraud_McCullough, Hudson and Schuster</td>\n",
       "      <td>user_461615966685</td>\n",
       "      <td>food_dining</td>\n",
       "      <td>2023-08-03 20:55:27.434522</td>\n",
       "      <td>35.37</td>\n",
       "      <td>2023-08-03 20:55:27.434</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fraud_Bernhard Inc</td>\n",
       "      <td>user_884240387242</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>2023-08-03 20:55:25.250398</td>\n",
       "      <td>78.23</td>\n",
       "      <td>2023-08-03 20:55:25.250</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fraud_Kub PLC</td>\n",
       "      <td>user_912293302206</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>2023-08-03 20:55:20.858521</td>\n",
       "      <td>11.68</td>\n",
       "      <td>2023-08-03 20:55:20.858</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fraud_Rempel Inc</td>\n",
       "      <td>user_650387977076</td>\n",
       "      <td>shopping_net</td>\n",
       "      <td>2023-08-03 20:55:18.432924</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2023-08-03 20:55:18.432</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fraud_Hackett-Lueilwitz</td>\n",
       "      <td>user_650387977076</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>2023-08-03 20:55:14.864775</td>\n",
       "      <td>22.72</td>\n",
       "      <td>2023-08-03 20:55:14.864</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fraud_Hintz-Bruen</td>\n",
       "      <td>user_650387977076</td>\n",
       "      <td>grocery_net</td>\n",
       "      <td>2023-08-03 20:55:12.984877</td>\n",
       "      <td>31.39</td>\n",
       "      <td>2023-08-03 20:55:12.984</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fraud_Jewess LLC</td>\n",
       "      <td>user_574612776685</td>\n",
       "      <td>shopping_pos</td>\n",
       "      <td>2023-08-03 20:55:10.962614</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2023-08-03 20:55:10.962</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                MERCHANT            USER_ID        CATEGORY  \\\n",
       "0      fraud_Haley, Jewess and Bechtelar  user_459842889956    shopping_pos   \n",
       "1                   fraud_Hamill-D'Amore  user_650387977076  health_fitness   \n",
       "2                      fraud_Durgan-Auer  user_394495759023        misc_net   \n",
       "3  fraud_McCullough, Hudson and Schuster  user_461615966685     food_dining   \n",
       "4                     fraud_Bernhard Inc  user_884240387242   gas_transport   \n",
       "5                          fraud_Kub PLC  user_912293302206   personal_care   \n",
       "6                       fraud_Rempel Inc  user_650387977076    shopping_net   \n",
       "7                fraud_Hackett-Lueilwitz  user_650387977076     grocery_pos   \n",
       "8                      fraud_Hintz-Bruen  user_650387977076     grocery_net   \n",
       "9                       fraud_Jewess LLC  user_574612776685    shopping_pos   \n",
       "\n",
       "                   TIMESTAMP    AMT        REQUEST_TIMESTAMP  IS_FRAUD  \\\n",
       "0 2023-08-03 20:55:36.156244   1.92  2023-08-03 20:55:36.156         0   \n",
       "1 2023-08-03 20:55:33.612956  53.08  2023-08-03 20:55:33.612         0   \n",
       "2 2023-08-03 20:55:30.557317   5.57  2023-08-03 20:55:30.557         0   \n",
       "3 2023-08-03 20:55:27.434522  35.37  2023-08-03 20:55:27.434         0   \n",
       "4 2023-08-03 20:55:25.250398  78.23  2023-08-03 20:55:25.250         0   \n",
       "5 2023-08-03 20:55:20.858521  11.68  2023-08-03 20:55:20.858         0   \n",
       "6 2023-08-03 20:55:18.432924   3.29  2023-08-03 20:55:18.432         0   \n",
       "7 2023-08-03 20:55:14.864775  22.72  2023-08-03 20:55:14.864         0   \n",
       "8 2023-08-03 20:55:12.984877  31.39  2023-08-03 20:55:12.984         0   \n",
       "9 2023-08-03 20:55:10.962614   1.52  2023-08-03 20:55:10.962         0   \n",
       "\n",
       "  TRANSACTION_AMOUNT_IS_HIGH  \n",
       "0                       None  \n",
       "1                       None  \n",
       "2                       None  \n",
       "3                       None  \n",
       "4                       None  \n",
       "5                       None  \n",
       "6                       None  \n",
       "7                       None  \n",
       "8                       None  \n",
       "9                       None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fv = tecton.get_workspace('demo-vince').get_feature_view('transaction_amount_is_high')\n",
    "\n",
    "fv.get_historical_features(transactions_query).to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17327962-90ac-4b1f-8c06-e662b83f3399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
